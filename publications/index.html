<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Zhen Zhu </title> <meta name="author" content="Zhen Zhu"> <meta name="description" content="Publications organized by research categories in reverse chronological order."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="Zhen Zhu"> <meta property="og:type" content="website"> <meta property="og:title" content="Zhen Zhu | Publications"> <meta property="og:url" content="https://jessemelpolio.github.io/publications/"> <meta property="og:description" content="Publications organized by research categories in reverse chronological order."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Publications"> <meta name="twitter:description" content="Publications organized by research categories in reverse chronological order."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+3:300,400,500,700|Source+Serif+4:400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?425de1c264012218017e3a120c428841"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jessemelpolio.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhen Zhu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Publications organized by research categories in reverse chronological order.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <p>My research spans several key areas in computer vision and machine learning, with a focus on developing practical and efficient solutions for real-world applications. Below are my publications organized by research category:</p> <div class="category-expansion"> <div class="category-expansion-header" onclick="togglePublicationsCategoryExpansion()"> <span>üîç Filter by Research Category</span> <i class="fas fa-chevron-down" id="publications-category-chevron" style="transform: rotate(180deg);"></i> </div> <div class="category-expansion-content expanded" id="publications-category-expansion-content"> <div class="category-buttons-container"> <button class="category-filter-btn active" onclick="filterPublicationsByCategory('all')" data-category="all"> All Categories </button> <button class="category-filter-btn" onclick="filterPublicationsByCategory('Multimodal Learning')" data-category="Multimodal Learning"> Multimodal Learning </button> <button class="category-filter-btn" onclick="filterPublicationsByCategory('Continual Learning')" data-category="Continual Learning"> Continual Learning </button> <button class="category-filter-btn" onclick="filterPublicationsByCategory('Image Generation')" data-category="Image Generation"> Image Generation </button> <button class="category-filter-btn" onclick="filterPublicationsByCategory('Object Detection/Segmentation')" data-category="Object Detection/Segmentation"> Object Detection/Segmentation </button> </div> </div> </div> <div class="all-publications-list"> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2019/05/07/5cd16f08e48e8.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Detection/Segmentation">Detection/Segmentation</span> </div> </div> </div> <div id="lyu2017auto" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2017</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#0891b2;font-weight:bold;border:1px solid #0891b2;"> <a href="https://icdar.org/" style="color: #0891b2 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">ICDAR</a> </abbr> </div> <div class="title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/1706.08789" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis </a> </div> <div class="author" style="margin-bottom: 8px;"> <a href="https://scholar.google.com/citations?user=whvv9NgAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Pengyuan Lyu</a>,¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a>,¬†<a href="http://www.yao.vision/" rel="external nofollow noopener" target="_blank">Cong Yao</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com/citations?user=N70UBoUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Tengteng Huang</a>,¬†and¬†<a href="http://eic.hust.edu.cn/professor/liuwenyu/" rel="external nofollow noopener" target="_blank">Wenyu Liu</a> </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/1706.08789" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="In this paper, we investigate the Chinese calligraphy synthesis problem: synthesizing Chinese calligraphy images with specified style from standard font(eg. Hei font) images. Recent works mostly follow the stroke extraction and assemble pipeline which is complex in the process and limited by the effect of stroke extraction. We treat the calligraphy synthesis problem as an image-to-image translation problem and propose a deep neural network based model which can generate calligraphy images from standard font images directly. Besides, we also construct a large scale benchmark that contains various styles for Chinese calligraphy synthesis. We evaluate our method as well as some baseline methods on the proposed dataset, and the experimental results demonstrate the effectiveness of our proposed model.">In this paper, we investigate the Chinese calligraphy synthesis problem: synthesizing Chinese calligraphy images with specified style from standard font(eg. Hei font) images. Recent works mostly follow the stroke extraction and assemble pipeline which is complex in the process and limited by the effect of stroke extraction. We treat the calligraphy synthesis problem as an image-to-image translation problem and propose a deep neural network based model which can generate calligraphy images from standard font images directly. Besides, we also construct a large scale benchmark that contains various styles for Chinese calligraphy synthesis. We evaluate our method as well as some baseline methods on the proposed dataset, and the experimental results demonstrate the effectiveness of our proposed model.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lyu2017auto</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lyu, Pengyuan and Bai, Xiang and Yao, Cong and Zhu, Zhen and Huang, Tengteng and Liu, Wenyu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICDAR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1095--1100}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Detection/Segmentation}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2024/09/29/zS5PvIaHB4Mx8re.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Detection/Segmentation">Detection/Segmentation</span> </div> </div> </div> <div id="xia2018dota" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2018</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#dc2626;font-weight:bold;border:1px solid #dc2626;"> <a href="https://cvpr.thecvf.com/" style="color: #dc2626 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">CVPR</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> DOTA: A Large-scale Dataset for Object Detection in Aerial Images </div> <div class="author selected-author" style="margin-bottom: 8px;"> <a href="http://www.captain-whu.com/xia.html" rel="external nofollow noopener" target="_blank">Gui-Song Xia</a>,¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a>,¬†Jian Ding,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://www.belongielab.org" rel="external nofollow noopener" target="_blank">Serge Belongie</a>,¬†<a href="https://www.cs.rochester.edu/u/jluo/" rel="external nofollow noopener" target="_blank">Jiebo Luo</a>,¬†Mihai Datcu,¬†Marcello Pelillo,¬†and¬†Liangpei Zhang </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/jessemelpolio/Faster_RCNN_for_DOTA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://captain-whu.github.io/DOTA/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth‚Äôs surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188,282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.">Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth‚Äôs surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188,282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xia2018dota</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DOTA: A Large-scale Dataset for Object Detection in Aerial Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xia, Gui-Song and Bai, Xiang and Ding, Jian and Zhu, Zhen and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3974--3983}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Detection/Segmentation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2019/05/07/5cd167d79fe98.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="zhou2018non" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2018</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> TOG </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> Non-stationary Texture Synthesis by Adversarial Expansion </div> <div class="author selected-author" style="margin-bottom: 8px;"> <a href="https://zhouyangvcc.github.io" rel="external nofollow noopener" target="_blank">Yang Zhou</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a>,¬†<a href="https://www.cs.huji.ac.il/~danix/" rel="external nofollow noopener" target="_blank">Dani Lischinski</a>,¬†<a href="https://www.cs.tau.ac.il/~dcor/" rel="external nofollow noopener" target="_blank">Daniel Cohen-Or</a> ,¬†and¬†Hui Huang </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/jessemelpolio/non-stationary_texture_syn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="http://vcc.szu.edu.cn/research/2018/TexSyn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="The real world exhibits an abundance of non-stationary textures. Examples include textures with large-scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large-scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.">The real world exhibits an abundance of non-stationary textures. Examples include textures with large-scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large-scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhou2018non</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Non-stationary Texture Synthesis by Adversarial Expansion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Yang and Zhu, Zhen and Bai, Xiang and Lischinski, Dani and Cohen-Or, Daniel and Huang, Hui}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (TOG)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--13}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM New York, NY, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2019/05/07/5cd16de14e40f.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Detection/Segmentation">Detection/Segmentation</span> </div> </div> </div> <div id="liao2018rotation" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2018</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#dc2626;font-weight:bold;border:1px solid #dc2626;"> <a href="https://cvpr.thecvf.com/" style="color: #dc2626 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">CVPR</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/1803.05265" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Rotation-sensitive Regression for Oriented Scene Text Detection </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <a href="https://mhliao.github.io/" rel="external nofollow noopener" target="_blank">Minghui Liao</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com/citations?user=zDhPcpUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baoguang Shi</a> ,¬†Gui-song Xia,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/1803.05265" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MhLiao/RRD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotation-sensitive features. The proposed method named Rotation-sensitive Regression Detector (RRD) achieves state-of-the-art performance on three oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection.">Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotation-sensitive features. The proposed method named Rotation-sensitive Regression Detector (RRD) achieves state-of-the-art performance on three oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liao2018rotation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rotation-sensitive Regression for Oriented Scene Text Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liao, Minghui and Zhu, Zhen and Shi, Baoguang and Xia, Gui-song and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5909--5918}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Detection/Segmentation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2019/07/23/5d3705a04461b68294.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Continual Learning">Continual Learning</span> </div> </div> </div> <div id="zhu2019asymmetric" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2019</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#7c3aed;font-weight:bold;border:1px solid #7c3aed;"> <a href="https://iccv.thecvf.com/" style="color: #7c3aed !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">ICCV</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/1908.07678" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Asymmetric Non-local Neural Networks for Semantic Segmentation </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com/citations?user=C04zJHEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Mengde Xu</a> ,¬†Song Bai,¬†<a href="https://scholar.google.com/citations?user=N70UBoUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Tengteng Huang</a>,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/1908.07678" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MendelXu/ANN.git" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation.">The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2019asymmetric</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Asymmetric Non-local Neural Networks for Semantic Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Xu, Mengde and Bai, Song and Huang, Tengteng and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{593--602}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Continual Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2019/05/07/5cd1692717221.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="zhu2019progressive" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="oral-presentation-badge" style="margin-bottom: 12px;"> <span style=" background: linear-gradient(135deg, #ff6b6b, #ee5a24); color: white; padding: 8px 16px; border-radius: 25px; font-size: 0.9em; font-weight: bold; text-transform: uppercase; letter-spacing: 1px; box-shadow: 0 4px 15px rgba(255, 107, 107, 0.4); border: 2px solid #fff; display: inline-block; "> üèÜ ORAL PRESENTATION </span> </div> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2019</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#dc2626;font-weight:bold;border:1px solid #dc2626;"> <a href="https://cvpr.thecvf.com/" style="color: #dc2626 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">CVPR</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/1904.03349" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Progressive Pose Attention Transfer for Person Image Generation </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com/citations?user=N70UBoUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Tengteng Huang</a>,¬†<a href="https://scholar.google.com/citations?user=zDhPcpUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baoguang Shi</a>,¬†Miao Yu,¬†Bofei Wang,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/1904.03349" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/tengteng95/Pose-Transfer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency.">This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2019progressive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Progressive Pose Attention Transfer for Person Image Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Huang, Tengteng and Shi, Baoguang and Yu, Miao and Wang, Bofei and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2347--2356}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/bNHFPMX9BVk" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2020/03/31/OT8u5NPAMykUsFL.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:Tyk-4Ss8FVUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Continual Learning">Continual Learning</span> </div> </div> </div> <div id="li2020semantic" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="oral-presentation-badge" style="margin-bottom: 12px;"> <span style=" background: linear-gradient(135deg, #ff6b6b, #ee5a24); color: white; padding: 8px 16px; border-radius: 25px; font-size: 0.9em; font-weight: bold; text-transform: uppercase; letter-spacing: 1px; box-shadow: 0 4px 15px rgba(255, 107, 107, 0.4); border: 2px solid #fff; display: inline-block; "> üèÜ ORAL PRESENTATION </span> </div> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2020</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#2563eb;font-weight:bold;border:1px solid #2563eb;"> <a href="https://eccv.ecva.net/" style="color: #2563eb !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">ECCV</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2002.10120" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Semantic Flow for Fast and Accurate Scene Parsing </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <a href="https://lxtgh.github.io/" rel="external nofollow noopener" target="_blank">Xiangtai Li</a>,¬†<a href="https://github.com/donnyyou" rel="external nofollow noopener" target="_blank">Ansheng You</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†Houlong Zhao,¬†Maoke Yang,¬†Kuiyuan Yang,¬†and¬†Yunhai Tong </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2002.10120" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used ‚Äì atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4% mIoU on Cityscapes with a frame rate of 26 FPS.">In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used ‚Äì atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4% mIoU on Cityscapes with a frame rate of 26 FPS.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2020semantic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantic Flow for Fast and Accurate Scene Parsing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Xiangtai and You, Ansheng and Zhu, Zhen and Zhao, Houlong and Yang, Maoke and Yang, Kuiyuan and Tong, Yunhai}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Continual Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2020/03/31/cFbdyOJ3Rj1xA8w.jpg" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:Y0pCki6q_DkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="zhu2020semantically" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2020</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#dc2626;font-weight:bold;border:1px solid #dc2626;"> <a href="https://cvpr.thecvf.com/" style="color: #dc2626 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">CVPR</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2003.12697" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Semantically Multi-modal Image Synthesis </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong> ,¬†Zhiliang Xu,¬†<a href="https://github.com/donnyyou" rel="external nofollow noopener" target="_blank">Ansheng You</a>,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2003.12697" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Seanseattle/SMIS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://seanseattle.github.io/SMIS/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="In this paper, we focus on semantically multi-modal image synthesis (SMIS) task, namely, generating multi-modal images at the semantic level. Previous work seeks to use multiple class-specific generators, constraining its usage in datasets with a small number of classes. We instead propose a novel Group Decreasing Network (GroupDNet) that leverages group convolutions in the generator and progressively decreases the group numbers of the convolutions in the decoder. Consequently, GroupDNet is armed with much more controllability on translating semantic labels to natural images and has plausible high-quality yields for datasets with many classes. Experiments on several challenging datasets demonstrate the superiority of GroupDNet on performing the SMIS task. We also show that GroupDNet is capable of performing a wide range of interesting synthesis applications.">In this paper, we focus on semantically multi-modal image synthesis (SMIS) task, namely, generating multi-modal images at the semantic level. Previous work seeks to use multiple class-specific generators, constraining its usage in datasets with a small number of classes. We instead propose a novel Group Decreasing Network (GroupDNet) that leverages group convolutions in the generator and progressively decreases the group numbers of the convolutions in the decoder. Consequently, GroupDNet is armed with much more controllability on translating semantic labels to natural images and has plausible high-quality yields for datasets with many classes. Experiments on several challenging datasets demonstrate the superiority of GroupDNet on performing the SMIS task. We also show that GroupDNet is capable of performing a wide range of interesting synthesis applications.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2020semantically</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantically Multi-modal Image Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Xu, Zhiliang and You, Ansheng and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5467--5476}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div id="xu2021facecontroller" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2021</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#be185d;font-weight:bold;border:1px solid #be185d;"> <a href="https://aaai.org/" style="color: #be185d !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">AAAI</a> </abbr> </div> <div class="title" style="margin-bottom: 8px;"> FaceController: Controllable Attribute Editing for Face in the Wild </div> <div class="author" style="margin-bottom: 8px;"> Zhiliang Xu,¬†Xiyu Yu,¬†Zhibin Hong,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†Junyu Han ,¬†Jingtuo Liu,¬†Errui Ding,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xu2021facecontroller</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FaceController: Controllable Attribute Editing for Face in the Wild}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Zhiliang and Yu, Xiyu and Hong, Zhibin and Zhu, Zhen and Han, Junyu and Liu, Jingtuo and Ding, Errui and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://i.loli.net/2021/04/21/FxvM76OEGsBHTz5.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="zhu2021progressive" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2021</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#1f2937;font-weight:bold;border:1px solid #1f2937;"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" style="color: #1f2937 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">TPAMI</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="https://ieeexplore.ieee.org/document/9384288" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Progressive and Aligned Pose Attention Transfer for Person Image Generation </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com/citations?user=N70UBoUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Tengteng Huang</a>,¬†<a href="https://scholar.google.com/citations?user=C04zJHEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Mengde Xu</a>,¬†<a href="https://scholar.google.com/citations?user=zDhPcpUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baoguang Shi</a>,¬†<a href="https://scholar.google.com/citations?user=sZL2R7YAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Wenqing Cheng</a>,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9384288" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. We design a progressive generator which comprises a sequence of transfer blocks. Each block performs an intermediate transfer step by modeling the relationship between the condition and the target poses with attention mechanism. Two types of blocks are introduced, namely Pose-Attentional Transfer Block (PATB) and Aligned Pose-Attentional Transfer Bloc (APATB). Compared with previous works, our model generates more photorealistic person images that retain better appearance consistency and shape consistency compared with input images. We verify the efficacy of the model on the Market-1501 and DeepFashion datasets, using quantitative and qualitative measures. Furthermore, we show that our method can be used for data augmentation for the person re-identification task, alleviating the issue of data insufficiency.">This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. We design a progressive generator which comprises a sequence of transfer blocks. Each block performs an intermediate transfer step by modeling the relationship between the condition and the target poses with attention mechanism. Two types of blocks are introduced, namely Pose-Attentional Transfer Block (PATB) and Aligned Pose-Attentional Transfer Bloc (APATB). Compared with previous works, our model generates more photorealistic person images that retain better appearance consistency and shape consistency compared with input images. We verify the efficacy of the model on the Market-1501 and DeepFashion datasets, using quantitative and qualitative measures. Furthermore, we show that our method can be used for data augmentation for the person re-identification task, alleviating the issue of data insufficiency.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2021progressive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Progressive and Aligned Pose Attention Transfer for Person Image Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Huang, Tengteng and Xu, Mengde and Shi, Baoguang and Cheng, Wenqing and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{44}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4306--4320}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPAMI.2021.3066681}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="liu2020wdnet" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2021</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#059669;font-weight:bold;border:1px solid #059669;"> <a href="https://wacv.thecvf.com/" style="color: #059669 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">WACV</a> </abbr> </div> <div class="title" style="margin-bottom: 8px;"> WDNet: Watermark-Decomposition Network for Visible Watermark Removal </div> <div class="author" style="margin-bottom: 8px;"> Yang Liu,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2020wdnet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{WDNet: Watermark-Decomposition Network for Visible Watermark Removal}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yang and Zhu, Zhen and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{WACV}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3685--3693}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2022/08/25/zf3RpCcSZGDhkUB.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:Se3iqnhoufwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="wu2022ccpl" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="oral-presentation-badge" style="margin-bottom: 12px;"> <span style=" background: linear-gradient(135deg, #ff6b6b, #ee5a24); color: white; padding: 8px 16px; border-radius: 25px; font-size: 0.9em; font-weight: bold; text-transform: uppercase; letter-spacing: 1px; box-shadow: 0 4px 15px rgba(255, 107, 107, 0.4); border: 2px solid #fff; display: inline-block; "> üèÜ ORAL PRESENTATION </span> </div> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2022</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#2563eb;font-weight:bold;border:1px solid #2563eb;"> <a href="https://eccv.ecva.net/" style="color: #2563eb !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">ECCV</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2207.04808" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <a href="https://scholar.google.com.hk/citations?user=VVU574oAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Zijie Wu</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†Junping Du,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2207.04808" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/JarrentWu1031/CCPL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="In this paper, we aim to devise a universally versatile style transfer method capable of performing artistic, photo-realistic, and video style transfer jointly, without seeing videos during training. Previous single-frame methods assume a strong constraint on the whole image to maintain temporal consistency, which could be violated in many cases. Instead, we make a mild and reasonable assumption that global inconsistency is dominated by local inconsistencies and devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local patches. CCPL can preserve the coherence of the content source during style transfer without degrading stylization. Moreover, it owns a neighbor-regulating mechanism, resulting in a vast reduction of local distortions and considerable visual quality improvement. Aside from its superior performance on versatile style transfer, it can be easily extended to other tasks, such as image-to-image translation. Besides, to better fuse content and style features, we propose Simple Covariance Transformation (SCT) to effectively align second-order statistics of the content feature with the style feature. Experiments demonstrate the effectiveness of the resulting model for versatile style transfer, when armed with CCPL.">In this paper, we aim to devise a universally versatile style transfer method capable of performing artistic, photo-realistic, and video style transfer jointly, without seeing videos during training. Previous single-frame methods assume a strong constraint on the whole image to maintain temporal consistency, which could be violated in many cases. Instead, we make a mild and reasonable assumption that global inconsistency is dominated by local inconsistencies and devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local patches. CCPL can preserve the coherence of the content source during style transfer without degrading stylization. Moreover, it owns a neighbor-regulating mechanism, resulting in a vast reduction of local distortions and considerable visual quality improvement. Aside from its superior performance on versatile style transfer, it can be easily extended to other tasks, such as image-to-image translation. Besides, to better fuse content and style features, we propose Simple Covariance Transformation (SCT) to effectively align second-order statistics of the content feature with the style feature. Experiments demonstrate the effectiveness of the resulting model for versatile style transfer, when armed with CCPL.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2022ccpl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zijie and Zhu, Zhen and Du, Junping and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{189--206}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=scZuJCXhL14" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="xu2022mobilefaceswap" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2022</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#be185d;font-weight:bold;border:1px solid #be185d;"> <a href="https://aaai.org/" style="color: #be185d !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">AAAI</a> </abbr> </div> <div class="title" style="margin-bottom: 8px;"> MobileFaceSwap: A Lightweight Framework for Video Face Swapping </div> <div class="author" style="margin-bottom: 8px;"> Zhiliang Xu,¬†Zhibin Hong,¬†Changxing Ding,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†Junyu Han ,¬†Jingtuo Liu,¬†and¬†Errui Ding </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xu2022mobilefaceswap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MobileFaceSwap: A Lightweight Framework for Video Face Swapping}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Zhiliang and Hong, Zhibin and Ding, Changxing and Zhu, Zhen and Han, Junyu and Liu, Jingtuo and Ding, Errui}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div id="peng2023comparing" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2023</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> JOV </abbr> </div> <div class="title" style="margin-bottom: 8px;"> Comparing Human Object Learning with Deep Neural Networks </div> <div class="author" style="margin-bottom: 8px;"> Yinuo Peng,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="http://dhoiem.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Derek Hoiem</a>,¬†and¬†Ranxiao Frances Wang </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Continual/Dynamic Learning">Continual/Dynamic Learning</span> </div> </div> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">peng2023comparing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Comparing Human Object Learning with Deep Neural Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Yinuo and Zhu, Zhen and Hoiem, Derek and Wang, Ranxiao Frances}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Vision}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4984--4984}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Continual/Dynamic Learning}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The Association for Research in Vision and Ophthalmology}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2024/09/29/2TaOMg9ZE4sBiCr.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="xie2023learning" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2023</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> arXiv </abbr> </div> <div class="title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2305.06200" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Learning in a single domain for non-stationary multi-texture synthesis </a> </div> <div class="author" style="margin-bottom: 8px;"> <a href="https://scholar.google.com/citations?user=jTrfuH8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xudong Xie</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com.hk/citations?user=VVU574oAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Zijie Wu</a> ,¬†Zhiliang Xu,¬†and¬†Yingying Zhu </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2305.06200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="This paper aims for a new generation task: non-stationary multi-texture synthesis, which unifies synthesizing multiple non-stationary textures in a single model. Most non-stationary textures have large scale variance and can hardly be synthesized through one model. To combat this, we propose a multi-scale generator to capture structural patterns of various scales and effectively synthesize textures with a minor cost. However, it is still hard to handle textures of different categories with different texture patterns. Therefore, we present a category-specific training strategy to focus on learning texture pattern of a specific domain. Interestingly, once trained, our model is able to produce multi-pattern generations with dynamic variations without the need to finetune the model for different styles. Moreover, an objective evaluation metric is designed for evaluating the quality of texture expansion and global structure consistency. To our knowledge, ours is the first scheme for this challenging task, including model, training, and evaluation. Experimental results demonstrate the proposed method achieves superior performance and time efficiency.">This paper aims for a new generation task: non-stationary multi-texture synthesis, which unifies synthesizing multiple non-stationary textures in a single model. Most non-stationary textures have large scale variance and can hardly be synthesized through one model. To combat this, we propose a multi-scale generator to capture structural patterns of various scales and effectively synthesize textures with a minor cost. However, it is still hard to handle textures of different categories with different texture patterns. Therefore, we present a category-specific training strategy to focus on learning texture pattern of a specific domain. Interestingly, once trained, our model is able to produce multi-pattern generations with dynamic variations without the need to finetune the model for different styles. Moreover, an objective evaluation metric is designed for evaluating the quality of texture expansion and global structure consistency. To our knowledge, ours is the first scheme for this challenging task, including model, training, and evaluation. Experimental results demonstrate the proposed method achieves superior performance and time efficiency.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xie2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning in a single domain for non-stationary multi-texture synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Xudong and Zhu, Zhen and Wu, Zijie and Xu, Zhiliang and Zhu, Yingying}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2305.06200}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2023/11/29/HOTwY1JzE8PqxoA.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="wu2023singleinsert" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2023</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> arXiv </abbr> </div> <div class="title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2310.08094" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing </a> </div> <div class="author" style="margin-bottom: 8px;"> <a href="https://scholar.google.com.hk/citations?user=VVU574oAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Zijie Wu</a>,¬†Chaohui Yu,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†Fan Wang,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2310.08094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://jarrentwu1031.github.io/SingleInsert-web/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="Recent progress in text-to-image (T2I) models enables high-quality image generation with flexible textual control. To utilize the abundant visual priors in the off-the-shelf T2I models, a series of methods try to invert an image to proper embedding that aligns with the semantic space of the T2I model. However, these image-to-text (I2T) inversion methods typically need multiple source images containing the same concept or struggle with the imbalance between editing flexibility and visual fidelity. In this work, we point out that the critical problem lies in the foreground-background entanglement when learning an intended concept, and propose a simple and effective baseline for single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage scheme. In the first stage, we regulate the learned embedding to concentrate on the foreground area without being associated with the irrelevant background. In the second stage, we finetune the T2I model for better visual resemblance and devise a semantic loss to prevent the language drift problem. With the proposed techniques, SingleInsert excels in single concept generation with high visual fidelity while allowing flexible editing. Additionally, SingleInsert can perform single-image novel view synthesis and multiple concepts composition without requiring joint training. To facilitate evaluation, we design an editing prompt list and introduce a metric named Editing Success Rate (ESR) for quantitative assessment of editing flexibility.">Recent progress in text-to-image (T2I) models enables high-quality image generation with flexible textual control. To utilize the abundant visual priors in the off-the-shelf T2I models, a series of methods try to invert an image to proper embedding that aligns with the semantic space of the T2I model. However, these image-to-text (I2T) inversion methods typically need multiple source images containing the same concept or struggle with the imbalance between editing flexibility and visual fidelity. In this work, we point out that the critical problem lies in the foreground-background entanglement when learning an intended concept, and propose a simple and effective baseline for single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage scheme. In the first stage, we regulate the learned embedding to concentrate on the foreground area without being associated with the irrelevant background. In the second stage, we finetune the T2I model for better visual resemblance and devise a semantic loss to prevent the language drift problem. With the proposed techniques, SingleInsert excels in single concept generation with high visual fidelity while allowing flexible editing. Additionally, SingleInsert can perform single-image novel view synthesis and multiple concepts composition without requiring joint training. To facilitate evaluation, we design an editing prompt list and introduce a metric named Editing Success Rate (ESR) for quantitative assessment of editing flexibility.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wu2023singleinsert</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zijie and Yu, Chaohui and Zhu, Zhen and Wang, Fan and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2310.08094}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2024/09/29/b2mCgAlS3r7y4Zk.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:KlAtU1dfN6UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Multimodal Learning">Multimodal Learning</span> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Continual/Dynamic Learning">Continual/Dynamic Learning</span> </div> </div> </div> <div id="zhu2024anytime" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="oral-presentation-badge" style="margin-bottom: 12px;"> <span style=" background: linear-gradient(135deg, #ff6b6b, #ee5a24); color: white; padding: 8px 16px; border-radius: 25px; font-size: 0.9em; font-weight: bold; text-transform: uppercase; letter-spacing: 1px; box-shadow: 0 4px 15px rgba(255, 107, 107, 0.4); border: 2px solid #fff; display: inline-block; "> üèÜ ORAL PRESENTATION </span> </div> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2024</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#2563eb;font-weight:bold;border:1px solid #2563eb;"> <a href="https://eccv.ecva.net/" style="color: #2563eb !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">ECCV</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2409.08518" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Anytime Continual Learning for Open Vocabulary Classification </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com.au/citations?user=e2u6hRoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yiming Gong</a>,¬†and¬†<a href="http://dhoiem.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Derek Hoiem</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2409.08518" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://zzhu.vision/anytime_continual_learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/jessemelpolio/AnytimeCL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="We propose an approach for anytime continual learning (AnytimeCL) for open vocabulary image classification. The AnytimeCL problem aims to break away from batch training and rigid models by requiring that a system can predict any set of labels at any time and efficiently update and improve when receiving one or more training samples at any time. Despite the challenging goal, we achieve substantial improvements over recent methods. We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task‚Äôs labels. We also propose an attention-weighted PCA compression of training features that reduces storage and computation with little impact to model accuracy. Our methods are validated with experiments that test flexibility of learning and inference.">We propose an approach for anytime continual learning (AnytimeCL) for open vocabulary image classification. The AnytimeCL problem aims to break away from batch training and rigid models by requiring that a system can predict any set of labels at any time and efficiently update and improve when receiving one or more training samples at any time. Despite the challenging goal, we achieve substantial improvements over recent methods. We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task‚Äôs labels. We also propose an attention-weighted PCA compression of training features that reduces storage and computation with little impact to model accuracy. Our methods are validated with experiments that test flexibility of learning and inference.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2024anytime</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Anytime Continual Learning for Open Vocabulary Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Gong, Yiming and Hoiem, Derek}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Multimodal Learning, Continual/Dynamic Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/gSOpLxQi8jg?si=Zhrhg7aXSqsFr9Gu" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2023/11/29/YcWr4FEsuafMimz.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> <div id="zhu2024consistent" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2024</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#059669;font-weight:bold;border:1px solid #059669;"> <a href="https://wacv.thecvf.com/" style="color: #059669 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">WACV</a> </abbr> </div> <div class="title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2307.01425" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Consistent Multimodal Generation via A Unified GAN Framework </a> </div> <div class="author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong> ,¬†<a href="https://yijunmaverick.github.io/" rel="external nofollow noopener" target="_blank">Yijun Li</a> ,¬†Weijie Lyu,¬†<a href="http://krsingh.cs.ucdavis.edu/" rel="external nofollow noopener" target="_blank">Krishna Kumar Singh</a>,¬†<a href="https://zhixinshu.github.io/" rel="external nofollow noopener" target="_blank">Zhixin Shu</a>,¬†<a href="https://storage.googleapis.com/pirk.io/index.html" rel="external nofollow noopener" target="_blank">S√∂ren Pirk</a>,¬†and¬†<a href="http://dhoiem.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Derek Hoiem</a> </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2307.01425" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/jessemelpolio/MultimodalGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="We investigate how to generate multimodal image outputs, such as RGB, depth, and surface normals, with a single generative model. The challenge is to produce outputs that are realistic, and also consistent with each other. Our solution builds on the StyleGAN3 architecture, with a shared backbone and modality-specific branches in the last layers of the synthesis network, and we propose per-modality fidelity discriminators and a cross-modality consistency discriminator. In experiments on the Stanford2D3D dataset, we demonstrate realistic and consistent generation of RGB, depth, and normal images. We also show a training recipe to easily extend our pretrained model on a new domain, even with a few pairwise data. We further evaluate the use of synthetically generated RGB and depth pairs for training or fine-tuning depth estimators.">We investigate how to generate multimodal image outputs, such as RGB, depth, and surface normals, with a single generative model. The challenge is to produce outputs that are realistic, and also consistent with each other. Our solution builds on the StyleGAN3 architecture, with a shared backbone and modality-specific branches in the last layers of the synthesis network, and we propose per-modality fidelity discriminators and a cross-modality consistency discriminator. In experiments on the Stanford2D3D dataset, we demonstrate realistic and consistent generation of RGB, depth, and normal images. We also show a training recipe to easily extend our pretrained model on a new domain, even with a few pairwise data. We further evaluate the use of synthetically generated RGB and depth pairs for training or fine-tuning depth estimators.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2024consistent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Consistent Multimodal Generation via A Unified GAN Framework}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Li, Yijun and Lyu, Weijie and Singh, Krishna Kumar and Shu, Zhixin and Pirk, S{\"o}ren and Hoiem, Derek}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{WACV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row selected-publication"> <div class="col-sm-5"> <div class="preview-container" style="display: flex; flex-direction: column; justify-content: flex-start; align-items: center; width: 100%; margin: 0; padding: 0;"> <img class="preview z-depth-1 rounded" src="https://s2.loli.net/2023/11/29/48ULFRYgJZKmTkv.png" style="width: 100%; height: auto; object-fit: contain; max-height: 250px; margin-bottom: 10px;"> <div class="scholar-counts" style="margin-bottom: 8px; text-align: left; width: 100%;"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nD6_ot4AAAAJ&amp;citation_for_view=nD6_ot4AAAAJ:hqOjcs7Dif8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations" style="max-width: 100%; height: auto;"> </a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Multimodal Learning">Multimodal Learning</span> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Continual/Dynamic Learning">Continual/Dynamic Learning</span> </div> </div> </div> <div id="zhu2023continual" class="col-sm-7"> <div class="publication-content" style="padding-left: 20px;"> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2024</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#ea580c;font-weight:bold;border:1px solid #ea580c;"> <a href="https://jmlr.org/tmlr/" style="color: #ea580c !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">TMLR</a> </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2307.01430" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Continual Learning in Open-vocabulary Classification with Complementary Memory Systems </a> </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong> ,¬†Weijie Lyu,¬†<a href="https://avaxiao.github.io" rel="external nofollow noopener" target="_blank">Yao Xiao</a>,¬†and¬†<a href="http://dhoiem.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Derek Hoiem</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2307.01430" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/jessemelpolio/TreeProbe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text='We introduce a method for flexible and efficient continual learning in open-vocabulary image classification, drawing inspiration from the complementary learning systems observed in human cognition. Specifically, we propose to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample‚Äôs class is within the exemplar classes. We also propose a "tree probe" method, an adaption of lazy learning principles, which enables fast learning from new examples with competitive accuracy to batch-trained linear models. We test in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories. Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness.'>We introduce a method for flexible and efficient continual learning in open-vocabulary image classification, drawing inspiration from the complementary learning systems observed in human cognition. Specifically, we propose to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample‚Äôs class is within the exemplar classes. We also propose a "tree probe" method, an adaption of lazy learning principles, which enables fast learning from new examples with competitive accuracy to batch-trained linear models. We test in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories. Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2023continual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Continual Learning in Open-vocabulary Classification with Complementary Memory Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Lyu, Weijie and Xiao, Yao and Hoiem, Derek}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{TMLR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Multimodal Learning, Continual/Dynamic Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="blume2024miracle" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2024</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color:#ffffff;color:#7c2d12;font-weight:bold;border:1px solid #7c2d12;"> <a href="https://www.acmmm.org/" style="color: #7c2d12 !important; text-decoration: none;" rel="external nofollow noopener" target="_blank">ACMM</a> </abbr> </div> <div class="title" style="margin-bottom: 8px;"> <a href="https://openreview.net/forum?id=dqmhErGul0" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> MIRACLE: An Online, Explainable Multimodal Interactive Concept Learning System </a> </div> <div class="author" style="margin-bottom: 8px;"> Ansel Blume,¬†Khanh Duy Nguyen,¬†Zhenhailong Wang,¬†Yangyi Chen,¬†Michal Shlapentokh-Rothman,¬†Xiaomeng Jin,¬†Jeonghwan Kim,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong> ,¬†Jiateng Liu ,¬†Kuan-Hao Huang, and <span class="more-authors" title="click to view 20 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '20 more authors' ? 'Mankeerat Sidhu, Xuanming Zhang, Vivian Liu, Raunak Sinha, Te-Lin Wu, Abhay Zala, Elias Stengel-Eskin, Da Yin, Yao Xiao, Utkarsh Mall, Zhou Yu, Kai-Wei Chang, Camille Cobb, Karrie Karahalios, Lydia Chilton, Mohit Bansal, Nanyun Peng, Carl Vondrick, Derek Hoiem, Heng Ji' : '20 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">20 more authors</span> </div> <div class="periodical" style="margin-bottom: 8px;"> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=dqmhErGul0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Multimodal Learning">Multimodal Learning</span> </div> </div> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">blume2024miracle</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MIRACLE: An Online, Explainable Multimodal Interactive Concept Learning System}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Blume, Ansel and Nguyen, Khanh Duy and Wang, Zhenhailong and Chen, Yangyi and Shlapentokh-Rothman, Michal and Jin, Xiaomeng and Kim, Jeonghwan and Zhu, Zhen and Liu, Jiateng and Huang, Kuan-Hao and Sidhu, Mankeerat and Zhang, Xuanming and Liu, Vivian and Sinha, Raunak and Wu, Te-Lin and Zala, Abhay and Stengel-Eskin, Elias and Yin, Da and Xiao, Yao and Mall, Utkarsh and Yu, Zhou and Chang, Kai-Wei and Cobb, Camille and Karahalios, Karrie and Chilton, Lydia and Bansal, Mohit and Peng, Nanyun and Vondrick, Carl and Hoiem, Derek and Ji, Heng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Multimedia}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Multimodal Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div id="zhu2025multimodal" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2025</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> Under Review </abbr> </div> <div class="title selected-title" style="margin-bottom: 8px;"> How To Teach Large Multimodal Models New Tricks? </div> <div class="author selected-author" style="margin-bottom: 8px;"> <strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†<a href="https://scholar.google.com.au/citations?user=e2u6hRoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yiming Gong</a>,¬†<a href="https://avaxiao.github.io" rel="external nofollow noopener" target="_blank">Yao Xiao</a> ,¬†Yaoyao Liu,¬†and¬†<a href="http://dhoiem.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Derek Hoiem</a> </div> <div class="periodical selected-periodical" style="margin-bottom: 8px;"> <span class="under-review-badge">Under Review</span> </div> <div class="periodical selected-periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Multimodal Learning">Multimodal Learning</span> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Continual/Dynamic Learning">Continual/Dynamic Learning</span> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="Large multimodal models (LMMs) are effective for many vision and language problems but may underperform in specialized domains such as object counting and clock reading. Fine-tuning improves target task performance but sacrifices generality, while retraining with an expanded dataset is expensive. We investigate how to teach LMMs new skills and domains, examining the effects of tuning different components and of multiple strategies to mitigate forgetting. We experiment by tuning on new target tasks singly or sequentially and measuring learning as target task performance and forgetting as held-out task performance. Surprisingly, we find that the self-attention projection layers in the language model of the tested LMM can be fine-tuned to learn without forgetting. Fine-tuning the MLP layers in the language model improves learning with much less forgetting than tuning the full model, and employing knowledge distillation regularization mitigates forgetting greatly. We will release code to foster reproducible research on continual adaptation of large multimodal models.">Large multimodal models (LMMs) are effective for many vision and language problems but may underperform in specialized domains such as object counting and clock reading. Fine-tuning improves target task performance but sacrifices generality, while retraining with an expanded dataset is expensive. We investigate how to teach LMMs new skills and domains, examining the effects of tuning different components and of multiple strategies to mitigate forgetting. We experiment by tuning on new target tasks singly or sequentially and measuring learning as target task performance and forgetting as held-out task performance. Surprisingly, we find that the self-attention projection layers in the language model of the tested LMM can be fine-tuned to learn without forgetting. Fine-tuning the MLP layers in the language model improves learning with much less forgetting than tuning the full model, and employing knowledge distillation regularization mitigates forgetting greatly. We will release code to foster reproducible research on continual adaptation of large multimodal models.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2025multimodal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How To Teach Large Multimodal Models New Tricks?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zhen and Gong, Yiming and Xiao, Yao and Liu, Yaoyao and Hoiem, Derek}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Multimodal Learning, Continual/Dynamic Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="gong2025instantedit" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2025</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> Under Review </abbr> </div> <div class="title" style="margin-bottom: 8px;"> InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow </div> <div class="author" style="margin-bottom: 8px;"> <a href="https://scholar.google.com.au/citations?user=e2u6hRoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yiming Gong</a>,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†and¬†Minjia Zhang </div> <div class="periodical" style="margin-bottom: 8px;"> <span class="under-review-badge">Under Review</span> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="We aim to tackle the challenge of fast text-guided image editing using diffusion models. The goal of this task is to perform a 4-step editing process on the image which closely follows the textual instruction while preserves vital information in the original image. We approach this challenge by revising on the two important steps in image editing, inversion and regeneration. Inspired by the formulation of RectifiedFlow based model, we design an inversion method, PerRFI, for this framework which induces less trajectory error during inversion. We further introduce a disentangled prompt guidance method, DPG, that controls image editability while providing better detail preservation than counterpart guidance strategy. Finally, we introduce ControlNet into the generation process using canny image as condition. This method helps to inject structural information into the model and also helps to remove distortions and artifacts. Our approach performs text-guided image editing in real-time, requiring only 8 numbers of functional evaluations (NFE), which takes 4 NFE in inversion and 4 NFE in generation. Our method is not only fast, but also achieves better qualitative and quantitative results comparing to other few-step methods.">We aim to tackle the challenge of fast text-guided image editing using diffusion models. The goal of this task is to perform a 4-step editing process on the image which closely follows the textual instruction while preserves vital information in the original image. We approach this challenge by revising on the two important steps in image editing, inversion and regeneration. Inspired by the formulation of RectifiedFlow based model, we design an inversion method, PerRFI, for this framework which induces less trajectory error during inversion. We further introduce a disentangled prompt guidance method, DPG, that controls image editability while providing better detail preservation than counterpart guidance strategy. Finally, we introduce ControlNet into the generation process using canny image as condition. This method helps to inject structural information into the model and also helps to remove distortions and artifacts. Our approach performs text-guided image editing in real-time, requiring only 8 numbers of functional evaluations (NFE), which takes 4 NFE in inversion and 4 NFE in generation. Our method is not only fast, but also achieves better qualitative and quantitative results comparing to other few-step methods.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gong2025instantedit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gong, Yiming and Zhu, Zhen and Zhang, Minjia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="xiao2025textregion" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2025</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> Under Review </abbr> </div> <div class="title" style="margin-bottom: 8px;"> TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models </div> <div class="author" style="margin-bottom: 8px;"> <a href="https://avaxiao.github.io" rel="external nofollow noopener" target="_blank">Yao Xiao</a>,¬†Qiqian Fu,¬†Heyi Tao ,¬†Yuqun Wu,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†and¬†<a href="http://dhoiem.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Derek Hoiem</a> </div> <div class="periodical" style="margin-bottom: 8px;"> <span class="under-review-badge">Under Review</span> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Multimodal Learning">Multimodal Learning</span> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="Image-text models excel at image-level tasks but struggle with detailed visual understanding. While these models provide strong visual-language alignment, segmentation models like SAM2 offer precise spatial boundaries for objects. To this end, we propose TextRegion, a simple, effective and training-free framework that combines the strengths of image-text models and SAM2 to generate powerful text-aligned region tokens. These tokens enable detailed visual understanding while preserving open-vocabulary capabilities. They can be directly applied to various downstream tasks, including open-world semantic segmentation, referring expression comprehension, and grounding. We conduct extensive evaluations and consistently achieve superior or competitive performance compared to state-of-the-art training-free methods. Additionally, our framework is compatible with many image-text models, making it highly practical and easily extensible as stronger models emerge.">Image-text models excel at image-level tasks but struggle with detailed visual understanding. While these models provide strong visual-language alignment, segmentation models like SAM2 offer precise spatial boundaries for objects. To this end, we propose TextRegion, a simple, effective and training-free framework that combines the strengths of image-text models and SAM2 to generate powerful text-aligned region tokens. These tokens enable detailed visual understanding while preserving open-vocabulary capabilities. They can be directly applied to various downstream tasks, including open-world semantic segmentation, referring expression comprehension, and grounding. We conduct extensive evaluations and consistently achieve superior or competitive performance compared to state-of-the-art training-free methods. Additionally, our framework is compatible with many image-text models, making it highly practical and easily extensible as stronger models emerge.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xiao2025textregion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xiao, Yao and Fu, Qiqian and Tao, Heyi and Wu, Yuqun and Zhu, Zhen and Hoiem, Derek}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Multimodal Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="zhu2025training" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2025</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> Under Review </abbr> </div> <div class="title" style="margin-bottom: 8px;"> Training-Free Geometric Image Editing on Diffusion Models </div> <div class="author" style="margin-bottom: 8px;"> Hanshen Zhu,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†Kaile Zhang,¬†<a href="https://scholar.google.com.au/citations?user=e2u6hRoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yiming Gong</a> ,¬†Yuliang Liu,¬†and¬†<a href="https://xbai.vlrlab.net" rel="external nofollow noopener" target="_blank">Xiang Bai</a> </div> <div class="periodical" style="margin-bottom: 8px;"> . *Joint first author <span class="under-review-badge">Under Review</span> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text="We tackle the problem of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, which proves difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity and edit precision, especially under demanding transformations.">We tackle the problem of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, which proves difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity and edit precision, especially under demanding transformations.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2025training</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Training-Free Geometric Image Editing on Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Hanshen and Zhu, Zhen and Zhang, Kaile and Gong, Yiming and Liu, Yuliang and Bai, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> <li> <div class="row"> <div id="cho2025zero" class="col-sm-12"> <div class="publication-content" style=""> <div class="publication-header" style="display: flex; align-items: center; gap: 10px; margin-bottom: 8px;"> <span class="year-badge badge badge-light" style="font-size: 1.0em; padding: 6px 10px; color: #1f2937; background-color: #ffffff; border: 1px solid #1f2937; flex-shrink: 0; font-weight: bold;">2025</span> <abbr class="badge rounded" style="font-size: 0.9em; padding: 6px 10px; flex-shrink: 0; background-color: #ffffff !important; color: #2563eb !important; border: 1px solid #000000 !important; font-weight: bold !important;"> Under Review </abbr> </div> <div class="title" style="margin-bottom: 8px;"> <a href="http://arxiv.org/abs/2412.13401" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank"> Zero-Shot Low Light Image Enhancement with Diffusion Prior </a> </div> <div class="author" style="margin-bottom: 8px;"> <a href="https://scholar.google.com/citations?user=abeSWQsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Joshua Cho</a>,¬†Sara Aghajanzadeh,¬†<strong style="font-weight: 900; text-decoration: underline; text-underline-offset: 2px;">Zhen Zhu</strong>,¬†and¬†D.A. Forsyth </div> <div class="periodical" style="margin-bottom: 8px;"> <span class="under-review-badge">Under Review</span> </div> <div class="periodical" style="margin-bottom: 10px;"> </div> <div class="links" style="margin-bottom: 10px;"> <a href="http://arxiv.org/abs/2412.13401" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2412.13401" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="category-tags" style="margin-bottom: 8px; text-align: left; width: 100%;"> <span class="badge badge-category clickable-tag" style="background-color: rgba(var(--global-theme-color-rgb), 0.1); color: var(--global-theme-color); border: 1px solid rgba(var(--global-theme-color-rgb), 0.3); margin: 2px; font-size: 0.75em; padding: 3px 6px; border-radius: 10px; cursor: pointer; transition: all 0.2s ease; display: inline-block;" data-category="Image Generation">Image Generation</span> </div> </div> </div> </div> <div class="col-12" style="margin-top: 15px;"> <div class="abstract-container" style="padding: 0; background-color: transparent; border: none; border-radius: 0;"> <p style="margin: 0;"> <strong>Abstract:</strong> <span class="abstract-text" data-full-text='In this paper, we present a simple yet highly effective "free lunch" solution for low-light image enhancement (LLIE), which aims to restore low-light images as if acquired in well-illuminated environments. Our method necessitates no optimization, training, fine-tuning, text conditioning, or hyperparameter adjustments, yet it consistently reconstructs low-light images with superior fidelity. Specifically, we leverage a pre-trained text-to-image diffusion prior, learned from training on a large collection of natural images, and the features present in the model itself to guide the inference, in contrast to existing methods that depend on customized constraints. Comprehensive quantitative evaluations demonstrate that our approach outperforms SOTA methods on established datasets, while qualitative analyses indicate enhanced color accuracy and the rectification of subtle chromatic deviations. Furthermore, additional experiments reveal that our method, without any modifications, achieves SOTA-comparable performance in the auto white balance (AWB) task.'>In this paper, we present a simple yet highly effective "free lunch" solution for low-light image enhancement (LLIE), which aims to restore low-light images as if acquired in well-illuminated environments. Our method necessitates no optimization, training, fine-tuning, text conditioning, or hyperparameter adjustments, yet it consistently reconstructs low-light images with superior fidelity. Specifically, we leverage a pre-trained text-to-image diffusion prior, learned from training on a large collection of natural images, and the features present in the model itself to guide the inference, in contrast to existing methods that depend on customized constraints. Comprehensive quantitative evaluations demonstrate that our approach outperforms SOTA methods on established datasets, while qualitative analyses indicate enhanced color accuracy and the rectification of subtle chromatic deviations. Furthermore, additional experiments reveal that our method, without any modifications, achieves SOTA-comparable performance in the auto white balance (AWB) task.</span> <span class="read-more-btn" onclick="toggleAbstract(this)" style="display: none; color: var(--global-theme-color); cursor: pointer; font-weight: 500; margin-left: 0.5rem;">Read more</span> </p> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cho2025zero</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Zero-Shot Low Light Image Enhancement with Diffusion Prior}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cho, Joshua and Aghajanzadeh, Sara and Zhu, Zhen and Forsyth, D.A.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">category</span> <span class="p">=</span> <span class="s">{Image Generation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <script>function toggleAbstract(t){try{const n=t.parentNode.querySelector(".abstract-text");if(!n)return void console.error("Abstract text element not found");const r=n.getAttribute("data-full-text");!n.classList.contains("truncated")?(n.classList.add("truncated"),n.textContent=r.substring(0,300)+"...",t.textContent="Read more"):(n.classList.remove("truncated"),n.textContent=r,t.textContent="Read less")}catch(e){console.error("Error in toggleAbstract:",e)}}document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".abstract-text").forEach(function(t){const e=t.getAttribute("data-full-text"),n=t.parentNode.querySelector(".read-more-btn");e&&e.length>300&&(t.classList.add("truncated"),t.textContent=e.substring(0,300)+"...",n&&(n.style.display="inline"))})});</script> <style>.oral-presentation-badge{text-align:left;margin-bottom:12px}.oral-presentation-badge span{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ee5a24);color:white;padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:bold;text-transform:uppercase;letter-spacing:1px;box-shadow:0 4px 15px rgba(255,107,107,0.4);border:2px solid #fff}@media(max-width:768px){.oral-presentation-badge span{font-size:.8em;padding:6px 12px;letter-spacing:.5px}}</style> </li> </ol> </div> </div> <style>.category-expansion{background:rgba(var(--global-theme-color-rgb),0.02);border:1px solid rgba(var(--global-theme-color-rgb),0.1);border-radius:12px;margin-bottom:2rem;overflow:hidden}.category-expansion-header{padding:15px 20px;cursor:pointer;display:flex;justify-content:space-between;align-items:center;background:rgba(var(--global-theme-color-rgb),0.05);border-bottom:1px solid rgba(var(--global-theme-color-rgb),0.1);font-weight:600;color:var(--global-theme-color);transition:all .2s ease}.category-expansion-header:hover{background:rgba(var(--global-theme-color-rgb),0.08)}.category-expansion-content{max-height:0;overflow:hidden;transition:all .3s ease;padding:0 20px}.category-expansion-content.expanded{max-height:200px;padding:15px 20px}.category-buttons-container{display:flex;flex-wrap:wrap;gap:8px;margin:0}.category-filter-btn{background:rgba(var(--global-theme-color-rgb),0.1);color:var(--global-theme-color);border:1px solid rgba(var(--global-theme-color-rgb),0.3);padding:8px 16px;border-radius:25px;font-size:.9em;font-weight:500;cursor:pointer;transition:all .2s ease;white-space:nowrap}.category-filter-btn:hover{background:rgba(var(--global-theme-color-rgb),0.2);transform:translateY(-1px);box-shadow:0 2px 8px rgba(var(--global-theme-color-rgb),0.2)}.category-filter-btn.active{background:var(--global-theme-color);color:white;border-color:var(--global-theme-color);box-shadow:0 2px 12px rgba(var(--global-theme-color-rgb),0.3)}.publications .abbr{display:flex;flex-direction:column;height:100%;.preview-container{width:100%;margin-bottom:1rem;img{width:100%;height:auto;max-height:250px;object-fit:contain;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.15);transition:transform .2s ease}img:hover{transform:scale(1.02)}}}.publications .teaser{.preview-container{width:100%;margin-bottom:1rem;img{width:100%;height:auto;max-height:250px;object-fit:contain;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.15)}}}.publications h2.bibliography{color:var(--global-theme-color);border-bottom:3px solid var(--global-theme-color);padding-bottom:.75rem;margin-top:3rem;margin-bottom:2rem;font-weight:700;font-size:2.5rem;position:relative;text-align:center}.publications h2.bibliography:first-child{margin-top:1.5rem}.publications h2.bibliography::after{content:'';position:absolute;bottom:-3px;left:50%;transform:translateX(-50%);width:60px;height:3px;background:linear-gradient(90deg,transparent,var(--global-theme-color),transparent);border-radius:2px}.all-publications-list .row{margin-bottom:2.5rem!important;padding:1.5rem;background:rgba(var(--global-bg-color),0.8);border-radius:12px;box-shadow:0 2px 15px rgba(0,0,0,0.08);transition:all .3s ease;border:1px solid rgba(var(--global-theme-color-rgb),0.1)}.all-publications-list .row:hover{transform:translateY(-2px);box-shadow:0 8px 25px rgba(0,0,0,0.12);border-color:rgba(var(--global-theme-color-rgb),0.2)}.all-publications-list .row.selected-publication{border-left:4px solid var(--global-theme-color);background:rgba(var(--global-theme-color-rgb),0.02)}.abstract-container{margin-top:1rem;padding:1rem;background:rgba(var(--global-theme-color-rgb),0.02);border-left:3px solid rgba(var(--global-theme-color-rgb),0.3);border-radius:0 8px 8px 0}.category-tags{margin:.75rem 0}.badge.badge-category{background-color:rgba(var(--global-theme-color-rgb),0.1)!important;color:var(--global-theme-color)!important;border:1px solid rgba(var(--global-theme-color-rgb),0.3)!important;margin:2px!important;font-size:.75em!important;padding:3px 6px!important;border-radius:10px!important;cursor:pointer!important;transition:all .2s ease!important;display:inline-block!important;text-decoration:none!important}.badge.badge-category:hover{transform:translateY(-1px);box-shadow:0 2px 8px rgba(var(--global-theme-color-rgb),0.3);background-color:rgba(var(--global-theme-color-rgb),0.2)!important;color:var(--global-theme-color)!important}.publications h3.bibliography{color:var(--global-text-color);border-bottom:1px solid var(--global-divider-color);padding-bottom:.3rem;margin-top:1.5rem;margin-bottom:1rem;font-weight:500}.bibliography>ol>li:first-child{margin-top:0}.bibliography ol{padding-left:0}.bibliography ol ol{padding-left:1.5rem}@media(max-width:768px){.category-expansion-header{padding:12px 15px;font-size:.9em}.category-expansion-content.expanded{padding:12px 15px}.category-buttons-container{gap:6px}.category-filter-btn{padding:6px 12px;font-size:.85em}.publications .abbr img,.publications .teaser img{max-height:180px!important}.publications h2.bibliography{font-size:2rem;margin-top:2rem;margin-bottom:1.5rem}.all-publications-list .row{padding:1rem;margin-bottom:2rem!important}.abstract-container{padding:.75rem}}</style> <script>function togglePublicationsCategoryExpansion(){const e=document.getElementById("publications-category-expansion-content"),o=document.getElementById("publications-category-chevron");e.classList.contains("expanded")?(e.classList.remove("expanded"),o.style.transform="rotate(0deg)"):(e.classList.add("expanded"),o.style.transform="rotate(180deg)")}function sortPublicationsYearHeaders(){const e=document.querySelector(".all-publications-list");if(!e)return void console.log("No publications container found");const o=e.querySelectorAll("h2.bibliography");if(console.log("Found year headers:",o.length),0===o.length)return void console.log("No year headers found");const t=[];o.forEach((e,o)=>{const n=e.textContent.trim(),l=parseInt(n);if(console.log(`Processing year header ${o}: "${n}" -> ${l}`),isNaN(l))return void console.log(`Skipping invalid year: ${n}`);const i=[e];let a=e.nextElementSibling;for(;a&&!a.matches("h2.bibliography");)i.push(a),a=a.nextElementSibling;console.log(`Year ${l} has ${i.length} elements`),t.push({year:l,elements:i})}),console.log("Elements before sorting:",t.map(e=>e.year)),t.sort((e,o)=>(console.log(`Comparing ${e.year} vs ${o.year}`),o.year-e.year)),console.log("Elements after sorting:",t.map(e=>e.year));const n=document.createElement("div");for(t.forEach((e,o)=>{console.log(`Appending year ${e.year} at position ${o}`),e.elements.forEach(e=>{n.appendChild(e)})});n.firstChild;)e.appendChild(n.firstChild);console.log("Publications year sorting completed")}function filterPublicationsByCategory(e){console.log(`\ud83d\udd0d FILTERING: Starting filter for category: "${e}"`),document.querySelectorAll(".category-filter-btn").forEach(e=>{e.classList.remove("active")});const o=document.querySelector(`[data-category="${e}"]`);o?(o.classList.add("active"),console.log(`\u2705 FILTERING: Active button set for ${e}`)):console.error(`\u274c FILTERING: Could not find button for category: ${e}`);const t=document.querySelectorAll(".all-publications-list ol.bibliography > li"),n=document.querySelectorAll(".all-publications-list h2.bibliography");if(console.log(`\ud83d\udcca FILTERING: Found ${t.length} publications and ${n.length} year headers`),0===t.length)return void console.error("\u274c FILTERING: No publications found! Check selector.");const l=new Set;t.forEach((o,t)=>{const n=o.id||`publication-${t}`;if(console.log(`\ud83d\udd0d FILTERING: Processing publication ${t} (${n})`),"all"===e){o.style.display="list-item",console.log(`\u2705 FILTERING: Showing publication ${t} (showing all)`);const e=findYearHeaderForPublicationElement(o);if(e){const o=e.textContent.trim();l.add(o)}}else{const n=o.querySelectorAll("[data-category]");console.log(`\ud83c\udff7\ufe0f FILTERING: Publication ${t}: Found ${n.length} category tags`);let i=!1;if(n.forEach((o,t)=>{const n=o.getAttribute("data-category"),l=o.textContent.trim();console.log(`   \ud83d\udcdd Tag ${t}: data-category="${n}", text="${l}"`),n!==e&&l!==e||(i=!0,console.log(`   \u2705 FILTERING: MATCH found for category: ${e}`))}),i){o.style.display="list-item",console.log(`\u2705 FILTERING: Showing publication ${t} - has category ${e}`);const n=findYearHeaderForPublicationElement(o);if(n){const e=n.textContent.trim();l.add(e)}}else o.style.display="none",console.log(`\u274c FILTERING: Hiding publication ${t} - no category ${e}`)}}),console.log("\ud83d\udcc5 FILTERING: Visible years:",Array.from(l)),n.forEach(e=>{const o=e.textContent.trim(),t=l.has(o);e.style.display=t?"block":"none",console.log(`\ud83d\udcc5 FILTERING: Year header ${o}: ${t?"visible":"hidden"}`)}),console.log(`\ud83c\udfaf FILTERING: Completed filtering for category: ${e}`)}function findYearHeaderForPublicationElement(e){let o=e.previousElementSibling;for(;o;){if("H2"===o.tagName&&o.classList.contains("bibliography"))return o;o=o.previousElementSibling}return null}document.addEventListener("DOMContentLoaded",function(){console.log("Publications page DOMContentLoaded: Starting initialization..."),setTimeout(()=>{console.log("Starting initial publications sort..."),sortPublicationsYearHeaders()},500),setTimeout(()=>{console.log("Starting backup publications sort..."),sortPublicationsYearHeaders()},2e3)}),window.addEventListener("load",function(){console.log("Publications page window loaded: Starting sort..."),setTimeout(()=>{sortPublicationsYearHeaders()},100)});</script> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Zhen Zhu. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script defer src="https://api.pirsch.io/pa.js" id="pianjs" data-code=""></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/footer-control.js"></script> </body> </html>